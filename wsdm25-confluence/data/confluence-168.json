{
    "id": "confluence-168",
    "title": "Analysis of Argo as a transport medium for VirtIO",
    "url": "https://openxt.atlassian.net/wiki/spaces/DC/pages/1333428225/Analysis+of+Argo+as+a+transport+medium+for+VirtIO",
    "content": "<p>Owned by Christopher Clark\nLast updated: Dec 29, 2020 by Christopher Clark\n\n</p><p>This document collates information obtained from inspection of the Linux kernel and Windows KVM VirtIO device drivers for running as guest workloads.</p><p></p><ac:structured-macro ac:macro-id=\"dac71dff-7ee8-4d90-ab92-c87fdb522a73\" ac:name=\"toc\" ac:schema-version=\"1\" data-layout=\"default\"></ac:structured-macro><h1>Analysis of the Linux VirtIO implementation and system structure</h1><p>Within the Linux kernel device driver implementations and QEMU device model.</p><h2>Driver structure</h2><p>There is a distinction between the general class of virtio device drivers, which provide function-specific logic implementing the front-end of virtual devices, and the transport virtio device drivers, which are responsible for device discovery and provision of virtqueues for data transport to the front-end drivers.</p><h2>VirtIO transport drivers</h2><p>There are several implementations of transport virtio drivers, designed to be interchangeable with respect to the virtio front-end drivers.</p><ul><li><p>virtio-pci-modern</p><ul><li><p>also: virtio-pci-legacy, intertwined with virtio-pci-modern via virtio-pci-common</p></li></ul></li><li><p>virtio-mmio</p></li><li><p>virtio-ccw : this is a s390-specific channel driver</p></li><li><p>virtio-dpa : new for 2020: virtual Data Path Acceleration</p></li></ul><p><em>There are also other instances of virtio transport drivers elsewhere in the Linux kernel:</em></p><ul><li><p><em>vop : virtio over PCIe</em></p></li><li><p><em>remoteproc : remote processor messaging transport</em></p></li><li><p><em>mellanox bluefield soc driver</em></p></li></ul><p><em>These are not relevant to the remainder of this document and so not discussed further.</em></p><p>These transport drivers communicate with backend implementations in the QEMU device model. Multiple transport drivers can operate concurrently in the same kernel without interference. The virtio-pci-modern transport driver is the most advanced implementation within the Linux kernel, so is appropriate for use for reference when building a new virtio-argo transport driver.</p><p>Each virtio device has a handle to the virtio transport driver where it originated, so the front-end virtio drivers can operate on devices from different transports on the same system without need for any different handling within the frontend driver.</p><h2>Virtio transport driver interface</h2><p>The interface that a transport driver must implement is defined in <code>struct virtio_config_ops</code> in <code>include/linux/virtio_config.h</code></p><p>The Virtio PCI driver populates the interface struct thus:</p><p></p><ac:structured-macro ac:macro-id=\"a0826897-e4ce-4f1f-bd9b-c8cc0ed32e1b\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[static const struct virtio_config_ops virtio_pci_config_ops = {\n  .get        = vp_get,\n  .set        = vp_set,\n  .generation = vp_generation,\n  .get_status = vp_get_status,\n  .set_status = vp_set_status,\n  .reset      = vp_reset,\n  .find_vqs   = vp_modern_find_vqs,\n  .del_vqs    = vp_del_vqs,\n  .get_features   = vp_get_features,\n  .finalize_features = vp_finalize_features,\n  .bus_name   = vp_bus_name,\n  .set_vq_affinity = vp_set_vq_affinity,\n  .get_vq_affinity = vp_get_vq_affinity,\n};]]></ac:plain-text-body></ac:structured-macro><h2>Device discovery and driver registration with the Virtio PCI transport driver</h2><p>Primarily implemented in: <code>drivers/virtio/virtio_pci_common.c</code></p><p>On a system that is using the Virtio PCI transport driver, Virtio devices that are exposed to the guest are surfaced as PCI devices with device identifiers that match those registered by the Virtio PCI transport driver.</p><p>ie. Each virtual device first surfaces on the PCI bus and then driver configuration propagates from that point.</p><p>The Virtio PCI transport driver registers as a PCI device driver, declaring the range of PCI IDs that it will match to claim devices. When such a device is detected and matched, <code>virtio_pci_probe</code> is called to initialize the device driver for it, which typically will proceed into <code>virtio_pci_modern_probe</code>, which implements quite a bit of validation and feature negociation logic. A key action in this function is to initialize the pointer to the struct that contains the transport ops function pointers: <code>vp_dev-&gt;vdev.config = &amp;virtio_pci_config_ops;</code></p><p>Back in <code>virtio_pci_probe</code>, the new virtual device is then registered in <code>register_virtio_device(&amp;vp_dev-&gt;vdev);</code> which calls <code>device_add</code>, which causes the kernel bus infrastructure to locate a matching front-end driver for this new device – the bus being the virtio bus -- and the front-end driver will then have its <code>probe</code> function called to register and activate the device with its actual front-end function.</p><p>Each front-end virtio driver needs to initialize the virtqueues that are for communication with the backend, and it does this via the methods in the device’s transport ops function pointer struct. Once the virtqueues are initialized, they are operated by the front-end driver via the standard virtqueue interfaces.</p><h2>Argo: Device discovery and driver registration with Virtio Argo transport</h2><p>A new Virtio transport driver, <code>virtio-argo</code>, should implement the <code>virtio_pci_config_ops</code> interface, and function as an alternative or complementary driver to Virtio PCI. In the same fashion as Virtio PCI, it will have responsibility for device discovery and invoke <code>device_add</code> for new virtual devices that it finds, but virtual devices will not be surfaced to the guest as PCI devices.</p><p>In modern Linux kernels, support for ACPI can be enabled without requiring support for PCI to be enabled.</p><ul><li><p>Introduced in this series: <a href=\"https://patchwork.kernel.org/cover/10738249/\">https://patchwork.kernel.org/cover/10738249/</a><br/>with subsequent fixes to it added later - search the git log for references to <code>5d32a66541c4</code>.</p></li></ul><p>Virtual devices to be discovered by the <code>virtio-argo</code> driver can be described in new ACPI tables that are provided to the guest. The tables can enumerate the devices and include the necessary configuration metadata for the front-ends to be matched, probed and configured for connection to the corresponding backends over the Argo transport.</p><p>ACPI has support for hotplug of devices, so precedent exists for being able to support handling dynamic arrival and removal of virtual devices via this interface.</p><p>With this implementation, there should be no requirement for PCI support to be enabled in the guest VM when using the <code>virtio-argo</code> transport, which stands in contrast to the existing Xen device drivers in PVH and HVM guests, where the Xen platform PCI device is needed, and to Virtio-PCI transport, which depends on PCI support.</p><h2>Virtqueues</h2><p>Virtqueues implement the mechanism for transport of data for virtio devices.</p><p>There are two supported formats for virtqueues, and each driver and each device may support either one or both formats:</p><ul><li><p>Split Virtqueues : the original format</p><ul><li><p>where the virtqueue is split into separate parts, each part being writable exclusively by either the driver or the device, but not both</p></li><li><p><a href=\"https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-240006\">https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-240006</a></p></li></ul></li><li><p>Packed Virtqueues : added in version 1.1 of the spec</p><ul><li><p>where the virtqueue functions with buffer areas that are writable by both the driver and device</p></li><li><p><a href=\"https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-610007\">https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-610007</a></p></li></ul></li></ul><p>Negociation between driver and device of which virtqueue format to operate occurs at device initialization.</p><h3>Current Linux driver implementations</h3><p>The packed virtqueue format is negociated by the <code>VIRTIO_F_RING_PACKED</code> feature bit. There are currently very few references to this value in the kernel. The <code>vring_create_virtqueue</code> function does provide an implementation for creation of packed vring structures and the vdpa transport driver offers the feature, but otherwise it appears that the kernel implementation of virtio device drivers all use the original split ring format.</p><h3>Current QEMU device implementations</h3><p>The general device-independent QEMU logic in <code>hw/virtio/virtio.c</code> has support for packed rings, but the device implementations do not appear to negociate it in the current implementation.</p><p>In <code>include/hw/virtio/virtio.h</code> there is a centrally-defined macro <code>DEFINE_VIRTIO_COMMON_FEATURES</code> used in the common <code>virtio_device_class_init</code> function that sets <code>VIRTIO_F_RING_PACKED</code> to false.</p><p><strong>Other virtio device implementations</strong></p><p>DPDK implements virtio devices that can use either packed or split virtqueues.</p><h2>Virtqueues : implemented by vrings</h2><p>The virtio transport driver is asked by a virtio front-end driver, on behalf of a device, to find the virtual queues needed to connect the driver to the backend. The function to obtain the virtqueues is accessed via the transport ops <code>find_vqs</code> function.</p><p>Each of the existing Linux kernel virtio transport drivers uses the <code>vring_create_virtqueue</code> function to provision vrings, which implement virtqueues.</p><p>Virtqueue definition in the Virtio 1.1 standard:</p><p><a href=\"https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-230005\">https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-230005</a></p><h3>Virtqueue interface</h3><p>See the virtqueue structure and functions in : <code>&lt;linux/virtio.h&gt;</code> and the virtqueue functions exported from <code>virtio_ring.c</code> , listed below. These functions are called directly - ie. not via function pointers in an ops structure - from call sites in the front-end virtio drivers. It does not appear that the virtqueue implementation (ie. vring) is practical to substitute for an alternative virtqueue implementation in a new transport driver.</p><p></p><ac:structured-macro ac:macro-id=\"a341ba3c-253e-4d10-b8ab-e4578a244776\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[struct virtqueue {\n  struct list_head list;\n  void (*callback)(struct virtqueue *vq);\n  const char *name;\n  struct virtio_device *vdev;\n  unsigned int index;\n  unsigned int num_free;\n  void *priv;\n};]]></ac:plain-text-body></ac:structured-macro><p></p><ac:structured-macro ac:macro-id=\"d60d6776-8650-45a8-9a75-832ee593c660\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[EXPORT_SYMBOL_GPL(virtio_max_dma_size);\nEXPORT_SYMBOL_GPL(virtqueue_add_sgs);\nEXPORT_SYMBOL_GPL(virtqueue_add_outbuf);\nEXPORT_SYMBOL_GPL(virtqueue_add_inbuf);\nEXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);\nEXPORT_SYMBOL_GPL(virtqueue_kick_prepare);\nEXPORT_SYMBOL_GPL(virtqueue_notify);\nEXPORT_SYMBOL_GPL(virtqueue_kick);\nEXPORT_SYMBOL_GPL(virtqueue_get_buf_ctx);\nEXPORT_SYMBOL_GPL(virtqueue_get_buf);\nEXPORT_SYMBOL_GPL(virtqueue_disable_cb);\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb_prepare);\nEXPORT_SYMBOL_GPL(virtqueue_poll);\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb);\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb_delayed);\nEXPORT_SYMBOL_GPL(virtqueue_detach_unused_buf);\nEXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);\nEXPORT_SYMBOL_GPL(virtqueue_get_avail_addr);\nEXPORT_SYMBOL_GPL(virtqueue_get_used_addr);\nEXPORT_SYMBOL_GPL(virtqueue_get_vring);\nEXPORT_SYMBOL_GPL(virtqueue_get_vring_size);\nEXPORT_SYMBOL_GPL(virtqueue_is_broken);]]></ac:plain-text-body></ac:structured-macro><h3>Vring interface</h3><p>These vring functions are exported:<br/></p><ac:structured-macro ac:macro-id=\"3d7ef74f-ad28-4578-b02b-403cd1888382\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[EXPORT_SYMBOL_GPL(vring_interrupt);\nEXPORT_SYMBOL_GPL(__vring_new_virtqueue);\nEXPORT_SYMBOL_GPL(vring_create_virtqueue);\nEXPORT_SYMBOL_GPL(vring_new_virtqueue);\nEXPORT_SYMBOL_GPL(vring_del_virtqueue);\nEXPORT_SYMBOL_GPL(vring_transport_features);\nEXPORT_SYMBOL_GPL(virtio_break_device);]]></ac:plain-text-body></ac:structured-macro><p>Of the above, only the <code>virtio_break_device</code> function is accessed by a non-transport virtio driver - it’s invoked by the <code>virtio_crypto_core</code> driver to force disable a device and assumes that the virtqueue implementation is a vring.</p><h2>Virtqueues: enabling Argo in the transport control path</h2><p>In the Virtio component architecture, the transport driver is responsible for mapping the virtqueue/vrings onto the transport medium. An Argo ring buffer can be treated as a DMA buffer for virtio.</p><p>Since both QEMU and the Linux kernel support use of split virtqueues, and the split virtqueue format separates the areas in the data structure for writes by the device and the driver, this is the correct virtqueue format to select for initial implementation of the Argo virtio transport. The split format will be assumed from this point onwards in this document.</p><h3>Vrings</h3><p>A vring is a variable-sized structure, allocated by the transport driver, with no requirement for a specific location within a memory page. There are alignment requirements for its data members. Some or all of the vring will typically be located within a shared memory region for other transports, but this is not the case for Argo.</p><p>The vring structure is defined in <code>uapi/linux/virtio_ring.h</code> :</p><ac:structured-macro ac:macro-id=\"8e13c8b6-7f31-4a2d-8132-0b8774aeb324\" ac:name=\"code\" ac:schema-version=\"1\"><ac:plain-text-body><![CDATA[/* Virtio ring descriptors: 16 bytes.  These can chain together via \"next\". */\nstruct vring_desc {\n    __virtio64 addr;            /* Address (guest-physical). */\n    __virtio32 len;\n    __virtio16 flags;\n    __virtio16 next;            /* We chain unused descriptors via this, too */\n};\n\nstruct vring_avail {\n    __virtio16 flags;\n    __virtio16 idx;\n    __virtio16 ring[];\n};\n\nstruct vring_used_elem {\n    __virtio32 id;  /* Index of start of used descriptor chain. */\n    __virtio32 len; /* Total len of the descriptor chain used (written to) */\n};\n\ntypedef struct vring_used_elem __attribute__((aligned(VRING_USED_ALIGN_SIZE)))\n    vring_used_elem_t;\n\nstruct vring_used {\n    __virtio16 flags;\n    __virtio16 idx;\n    vring_used_elem_t ring[];\n};\n\n#define VRING_AVAIL_ALIGN_SIZE 2\n#define VRING_USED_ALIGN_SIZE 4\n#define VRING_DESC_ALIGN_SIZE 16\n\ntypedef struct vring_desc __attribute__((aligned(VRING_DESC_ALIGN_SIZE)))\n    vring_desc_t;\ntypedef struct vring_avail __attribute__((aligned(VRING_AVAIL_ALIGN_SIZE)))\n    vring_avail_t;\ntypedef struct vring_used __attribute__((aligned(VRING_USED_ALIGN_SIZE)))\n    vring_used_t;\n\n/* size in bytes values below are taken from the spec document */\nstruct vring {\n    unsigned int num;      /* required to be a power of 2 */\n    vring_desc_t *desc;    /* size in bytes: 16 * num */\n    vring_avail_t *avail;  /* size in bytes: 6 + (2 * num) */\n    vring_used_t *used;    /* size in bytes: 6 + (8 * num) */\n};]]></ac:plain-text-body></ac:structured-macro><p>The standard layout for a vring contains an array of descriptors, each being 16 bytes, and the availability of each descriptor for writing into is tracked in a separate array within the ring structure.</p><p>For the driver to send a buffer to a device:</p><ul><li><p>one or more slots are filled in the<code> vring-&gt;desc</code> table, with multiple slots chained together using <code>vring_desc-&gt;next</code></p></li><li><p>the index of the descriptor is written to the available ring</p></li><li><p>the device is notified</p></li></ul><p>When a device has finished with a buffer that was supplied by the driver:</p><ul><li><p>the index of the descriptor is written to the used ring</p></li><li><p>the driver is notified of the used buffer</p></li></ul><p>Write access within the vring structure:</p><ul><li><p>The descriptor table is only written to by the driver and read by the device.</p></li><li><p>The available ring is only written to by the driver and read by the device.</p></li><li><p>The used ring is only written to by the device and read by the driver.</p></li></ul><h4>Vring descriptors</h4><p>Each vring descriptor describes a buffer that is either write-only (<code>VRING_DESC_F_WRITE</code> set in flags) or read-only (<code>VRING_DESC_F_WRITE</code> clear in flags) for the device, with a guest-physical <code>addr</code> and a size in bytes indicated in <code>len</code>, optionally chained together via the <code>next</code> field.</p><p>Indirect descriptors enable a buffer to describe a list of buffer descriptors, to increase the maximum capacity of a ring and support efficient dispatch of large requests.</p><h3>Argo rings</h3><p>An Argo ring is also a variable-sized structure, allocated by the transport driver, and must be aligned at the head of a page of memory, and it is used to receive data transmitted from other domains. The ring header is 64 bytes.</p><ac:structured-macro ac:macro-id=\"3dd358ba-9c6e-4e06-ba6c-b7acaa4e2fa7\" ac:name=\"code\" ac:schema-version=\"1\"><ac:plain-text-body><![CDATA[typedef struct xen_argo_ring\n{\n    uint32_t rx_ptr;\n    uint32_t tx_ptr;\n    uint8_t reserved[56];             /* Reserved */\n    uint8_t ring[XEN_FLEX_ARRAY_DIM];\n} xen_argo_ring_t;]]></ac:plain-text-body></ac:structured-macro><p>An Argo ring contains slots, each being 16 bytes. Each message that is sent is rounded up in size to the next slot boundary, and has a 16 byte message header inserted ahead of each message in the destination Argo ring.</p><h3>Enabling remote writes for device backends into vrings using Argo</h3><h4>The used ring</h4><p>The remote domain which implements the backend device that the virtio argo transport communciates with needs to be able to write into the ‘used’ ring within the vring, as it consumes the buffers supplied by the driver. It must also not overwrite the ‘available’ ring that typically immediate precedes it in the vring structure, so this is achieved by ensuring that the ‘used' ring starts on a separate memory page.</p><p>The following data structure allocation and positioning will support remote writes into the virtio used ring without requiring copies out of the receiving Argo ring to populate the vring used ring:</p><ul><li><p>The vring is allocated so that the ‘used’ ring, within the vring structure, starts on a page boundary.</p><ul><li><p><em>This requirement is necessary regardless of page size - ie. true for 4k and 64k page sizes</em></p></li></ul></li><li><p>A single page is allocated to contain the Argo ring header and the initial region of the Argo ring, which will include (ample) space for the Argo message header of 16 bytes</p></li><li><p>When the Argo ring is registered with the hypervisor, it is registered as a multi-page ring</p><ul><li><p>The first page is the one allocated for containing the Argo ring header and space for an Argo message header</p></li><li><p>The subsequent pages of the Argo ring are all those which contain the vring ‘used’ ring</p></li></ul></li><li><p>The Argo ring is sized so that the end of the Argo ring is at the end of the vring ‘used’ ring, which will prevent overwrites beyond the used ring.</p></li></ul><p>This will enable the remote Argo sender domain to transmit directly into the used vring, while ensuring that the Argo ring header and message header are safely outside the vring and do not interfere with its operation. The sender needs to ensure that the message header for any transmits is always written outside of the vring used ring and into the first page of the Argo ring.</p><p>The current logic for initializing vrings when virtqueues are created by existing transport drivers, in <code>vring_create_virtqueue</code>, assumes that the vring contents are contiguous in virtual memory – see:</p><ac:structured-macro ac:macro-id=\"c9a59938-b3cb-4dbb-ac9f-051af50e0a73\" ac:name=\"code\" ac:schema-version=\"1\"><ac:plain-text-body><![CDATA[static inline void vring_init(struct vring *vr, unsigned int num, void *p,\n                  unsigned long align)\n{\n    vr->num = num;\n    vr->desc = p;\n    vr->avail = (struct vring_avail *)((char *)p + num * sizeof(struct vring_desc));\n    vr->used = (void *)(((uintptr_t)&vr->avail->ring[num] + sizeof(__virtio16)\n        + align-1) & ~(align - 1));\n}\n]]></ac:plain-text-body></ac:structured-macro><p>A new virtio-argo transport driver is not necessarily required to use this interface for initializing vrings, but enabing reuse of the existing functions will avoid duplicating common aspects of them.</p><h4>The available ring and descriptor ring</h4><p>The memory pages occupied by the available ring and descriptor ring can be transmitted via Argo when their contents are updated.</p><p><em>To be verified: </em>A virtio front-end driver will invoke <code>virtqueue_notify</code> to notify the other end of changes, which uses a function pointer in the virtqueue that is set by the transport driver, which can invoke Argo sendv.</p><h3>Vring use of the DMA interface provided by the virtio transport driver</h3><p>The logic in <code>vring_use_dma_api</code> ensures that Xen domains always used the Linux DMA API for allocating the memory for vrings. Each DMA operation is invoked with a handle to the virtual device’s parent: <code>vdev-&gt;dev.parent</code> , which is set by the virtio transport driver – see <code>virtio_pci_probe</code> for example - and so can be set to be that virtio transport driver.</p><p>The memory to be used for the vring structure is then allocated via the DMA API interface that has been provided. With a new implementation of the DMA API interface, the <code>virtio-argo</code> transport driver will be able to allocate the vrings with memory layout described in the section above, to support registration of a corresponding Argo ring for access to the vring’s used ring.</p><h2>Virtqueues: enabling Argo in the data path</h2><p>The transmit sequence for sending data via the virtqueue looks like:</p><ul><li><p>virtqueue_add_outbuf</p></li><li><p>virtqueue_kick_prepare <em>(or virtqueue_kick, which performs this and virtqueue_notify)</em></p></li><li><p>virtqueue_notify</p></li></ul><p>and when the buffers have been processed by the remote end, they will be indicated as used:</p><ul><li><p>virtqueue_poll  - returns true if there are pending used buffers to process in the virtqueue</p></li><li><p>vring_interrupt → call to the registered virtqueue callback</p></li></ul><p>The receive sequence for incoming data via the virtqueue looks like:</p><ul><li><p>virtqueue_add_inbuf</p></li><li><p>virtqueue_kick_prepare <em>(or virtqueue_kick, which performs this and virtqueue_notify)</em></p></li><li><p>virtqueue_notify</p></li><li><p>virtqueue_poll  - returns true if there are pending used buffers to process in the virtqueue</p></li><li><p>vring_interrupt → call to the registered virtqueue callback</p></li></ul><p>Each of the virtio ring operations for managing exposure of buffers from the front-end virtio driver passes through the DMA operations functions struct provided by the device’s transport driver.</p><h3>DMA operations</h3><ac:structured-macro ac:macro-id=\"ebf14a96-8d97-4d90-9abc-e1139cbd9886\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[struct dma_map_ops {\n    ... alloc\n    ... free\n    ... mmap\n    ... get_sgtable\n    ... map_page\n    ... unmap_page\n    ... map_sg\n    ... unmap_sg\n    ... map_resource\n    ... unmap_resource\n    ... sync_single_for_cpu\n    ... sync_single_for_device\n    ... sync_sg_for_cpu\n    ... sync_sg_for_device\n    ... cache_sync\n    ... dma_supported\n    ... get_required_mask\n    ... max_mapping_size\n    ... get_merge_boundary\n};]]></ac:plain-text-body></ac:structured-macro><h3>Flow of front-end operations on the virtqueue</h3><ac:structured-macro ac:macro-id=\"7ef1017f-1a00-47d3-a65a-9494be3eae40\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c</ac:parameter><ac:plain-text-body><![CDATA[virtqueue_add_sgs\n  → virtqueue_add\n    → virtqueue_add_split\n      → vring_map_one_sg( ... DMA_TO_DEVICE)\n        → dma_map_page\n      → vring_map_one_sg( ... DMA_FROM_DEVICE)\n        → dma_map_page\n      → vring_map_single( ... DMA_TO_DEVICE)\n        → dma_map_single\n      → virtqueue_kick\n\n\nvirtqueue_add_outbuf → virtqueue_add …\nvirtqueue_add_inbuf → virtqueue_add …\nvirtqueue_add_inbuf_ctx → virtqueue_add …\n]]></ac:plain-text-body></ac:structured-macro><p>The DMA map operations return a <code>dma_addr_t</code> which is then translated to a 64bit value to insert into the virtio descriptor struct written into the vring’s descriptor ring.</p><h3>Inbound receive buffers</h3><p>For <code>dma_map_page</code> with a caller requesting the <code>DMA_FROM_DEVICE</code> direction:</p><ul><li><p>The transport driver’s DMA API implementation can register a new Argo ring, so that the remote device can invoke the hypervisor to write directly into the buffer</p><ul><li><p>the first page of the new ring needs to be a separate page to hold the ring header and incoming message header</p></li><li><p>the subsequent pages of the ring should be the memory supplied by the caller for the buffer to receive into</p></li><li><p>the DMA address returned for insertion into the virtqueue’s descriptor ring should encode the Argo ring address</p></li></ul></li><li><p>An Argo notification (currently VIRQ) will occur when data has been sent to the buffer</p></li><li><p>The Argo ring can be unregistered when the buffer has been reported as used via the virtqueue’s used ring. It is safe to do so at any point since the driver domain owns the ring memory; it will just prevent any further writes to it from the remote sender once unregistered.</p></li><li><p>At ring registration, the Argo ring’s <code>rx_ptr</code> can be initialized to point to the start of the receive buffer supplied by the caller, minus space for the message header, so that the data written by the Argo <code>sendv</code> by the hypervisor on behalf of the remote device will go directly into the buffer where it is needed.</p></li></ul><h3>Oubound transmit buffers</h3><p>Handling <code>dma_map_page</code> with a caller requesting the <code>DMA_TO_DEVICE</code> direction:</p><ul><li><p>When the remote implementation of the Argo virtio device initializes state, it must register an Argo ring to contain a buffer for receiving incoming data on behalf of the device.</p></li><li><p>The address for the device’s incoming buffer ring must be communicated to the Argo virtio transport driver within the guest, as part of the device discovery protocol.</p></li><li><p>The transport driver’s DMA API implementation will invoke the Argo <code>sendv</code> operation to transmit the data provided in the <code>dma_map_page</code> into the Argo ring provided by the remote device.</p></li><li><p>When the <code>sendv</code> is issued, the message needs to include an identifier that is then encoded into the DMA address that is returned by <code>dma_map_page</code> for passing into the virtqueue’s descriptor ring.</p></li></ul><h1>Analysis of the Windows VirtIO implementation</h1><p>The Windows VirtIO device drivers have the transport driver abstraction and separate driver structure, with the <a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/master/VirtIO/VirtIORing.c\">core virtqueue data structures</a>, as the Linux VirtIO kernel implementation does.</p><p>The current <a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows\">kvm-guest-drivers-windows</a> repository includes <a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/master/VirtIO/VirtIOPCIModern.c\">virtio-pci-modern</a> and <a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/master/VirtIO/VirtIOPCILegacy.c\">virtio-pci-legacy</a> transports. virtio-mmio appears to be absent.</p><ul><li><p>The virtio-pci-modern driver implements a virtio_device_ops interface, very similar to the virtio_config_ops interface in the Linux drivers:<br/><a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/2d23ce407cada8fd736949921224691725049bd4/VirtIO/VirtIOPCIModern.c#L413\">https://github.com/virtio-win/kvm-guest-drivers-windows/blob/2d23ce407cada8fd736949921224691725049bd4/VirtIO/VirtIOPCIModern.c#L413</a></p></li><li><p>The virtio-pci-modern transport implements both packed and split virtqueue support.</p></li><li><p>The logic in <code>virtqueue_add_buf_split</code> does not implement the same DMA translation interface at the point where physical addresses are taken from the data provided by each client virtio driver; <em>however</em>:</p><ul><li><p>The VirtIO WDF common driver support code implements a DMA memory allocation interface:</p><ul><li><p><a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/VirtIO/WDF/VirtIOWdf.h#L153\">https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/VirtIO/WDF/VirtIOWdf.h#L153</a></p></li><li><p><a data-card-appearance=\"inline\" href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/VirtIO/WDF/Dma.c\">https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/VirtIO/WDF/Dma.c</a> </p><ul><li><p>With IOMMU support – see this commit:<br/><a data-card-appearance=\"inline\" href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/commit/a9401e828787d044c14d084ebed9015e1b686db5\">https://github.com/virtio-win/kvm-guest-drivers-windows/commit/a9401e828787d044c14d084ebed9015e1b686db5</a> </p></li></ul></li></ul></li><li><p>Storage: <a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/commit/17be9030aa45f260e306c49394f0ca2c74ad92a2\">comments in the git history</a> of the <code>viostor</code> driver indicate that the client virtio driver uses the Windows DMA API for memory allocation related to data transfer</p></li><li><p>Network: the network driver uses the NDIS framework, which does interact with a DMA-aware allocator</p><ul><li><p>ref: <a href=\"https://docs.microsoft.com/en-us/windows-hardware/drivers/network/ndis-scatter-gather-dma\">https://docs.microsoft.com/en-us/windows-hardware/drivers/network/ndis-scatter-gather-dma</a></p></li></ul></li><li><p>Input: The input driver uses the VirtIO WDF DMA-aware allocator:</p><ul><li><p><a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/vioinput/sys/Device.c#L478\">https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/vioinput/sys/Device.c#L478</a></p></li><li><p><a href=\"https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/vioinput/sys/HidKeyboard.c#L254\">https://github.com/virtio-win/kvm-guest-drivers-windows/blob/8098cb90a830c05ff42808ef467572c93aa64552/vioinput/sys/HidKeyboard.c#L254</a></p></li></ul></li></ul></li></ul><p>In summary: it looks promising that leverage of DMA translation interfaces to allow for insertion of the Argo transport in the same fashion as the Linux VirtIO-Argo design may be feasible with the Windows VirtIO driver implementations, but it is not yet clear whether the client VirtIO drivers may need to be recompiled, since the DMA address translation is performed differently in each of the storage, networking and input drivers inspected.</p><h1>References:</h1><ul><li><p>Linux kernel, version 5.8.0-rc7, cs:6ba1b005<br/><a href=\"git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git\">git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git</a></p></li><li><p>The Virtio 1.1 specification, OASIS<br/><a href=\"https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html\">https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html</a></p></li><li><p>Qemu, version 5.0.92 cs:14486297<br/><a data-card-appearance=\"inline\" href=\"https://github.com/qemu/qemu.git\">https://github.com/qemu/qemu.git</a> </p></li><li><p>DPDK, version 20.08-rc3, cs:6636b04b<br/><a href=\"git://dpdk.org/dpdk\">git://dpdk.org/dpdk</a></p></li><li><p>virtio: Towards a De-Facto Standard For Virtual I/O Devices<br/>Rusty Russell, IBM OzLabs; ACM SIGOPS Operating Systems Review, 2008.<br/><a href=\"https://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf\">https://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf</a></p></li><li><p>Windows KVM Virtio device drivers source code<br/><a data-card-appearance=\"inline\" href=\"https://github.com/virtio-win/kvm-guest-drivers-windows\">https://github.com/virtio-win/kvm-guest-drivers-windows</a> <br/></p></li></ul><hr/><h1>Notes from Virtio Argo planning meeting, July 2020</h1><h2>Call on Friday 17th July</h2><p>Attendees:</p><ul><li><p>Christopher Clark, Rich Persaud; MBO/BAE</p></li><li><p>Daniel Smith; Apertus Solutions</p></li><li><p>Eric Chanudet, Nick Krasnoff;  AIS Boston</p></li></ul><p>Actions:</p><ul><li><p>Christopher to produce a draft plan, post to Wire, discuss and then<br/>put it onto a private page on the OpenXT confluence wiki</p></li><li><p>Christopher and Eric to commence building a prototype as funding permits,<br/>with Daniel providing design input and review</p></li></ul><h2>Topic: Argo as a transport for Virtio</h2><p>Background: the Virtio implementation in the Linux kernel is stacked:</p><ul><li><p>there are many virtio drivers for different virtual device functions</p></li><li><p>each virtio driver uses a common in-kernel virtio transport API,<br/>and there are multiple alternative transport implementations</p></li><li><p>transport negociation proceeds, then driver-level negociation<br/>to establish the frontend-backend communication for each driver</p></li></ul><p>KVM uses Qemu to provide backend implementations for devices.<br/>To implement Argo as a Virtio transport, will need to make Qemu Argo-aware.</p><p>Typically a PCI device is emulated by Qemu, on a bus provided to the guest.<br/>The Bus:Device:Function (bdf) triple maps to Qemu; the guest loads the PCI<br/>virio transport when this PCI device with that bdf is detected.</p><p>For Xen, this is OK for HVM guests, but since the platform PCI device is not<br/>exposed to PVH (or PV) guests, not a sufficient method for those cases.</p><h3>Discussion notes</h3><p>The existing main Xen PCI code is small, used for discovery.<br/>vPCI is experimental full emulation of PCI within Xen -- Daniel does not<br/>favour this direction.</p><p>Paul Durrant has a draft VFIO implementation plan, previously circulated.<br/>Paul is working on emulating PCI devices directly via a IOReq server (see: \"<a href=\"https://xenbits.xen.org/gitweb/?p=people/pauldu/demu.git;a=tree\">demu</a>\").</p><p>Daniel is interested in the Linux kernel packet mmap, to avoid one world-switch<br/>when processing data.</p><p>Christopher raised the Cambridge Argo port-connection design for external<br/>connection of Argo ports between VMs.</p><p>Eric raised the vsock-argo implementation; discussion mentioned the Argo development wiki page that describes requirements and plan that has previously been discussed and agreed for future Argo Linux driver development, which needs to include a non-vsock interface for administrative actions such as runtime firewall configuration.</p><p>Daniel is positively inclined towards using ACPI tables for discovery<br/>and notes the executable ACPI table feature, via AML.</p><p>Naming: an Argo transport for virtio in the Linux kernel would, to match the<br/>existing naming scheme, be: virtio-argo.c</p><p>Group inclined not to require XenStore.</p><ul><li><p>Relevant to dom0less systems where XenStore is not necessary present.</p></li><li><p>Don't want to introduce another dependency on it.</p></li><li><p>uXen demonstrates that it is not a requirement.</p></li><li><p>XenStore is a performance bottleneck (see NoXS work), bad interface, etc.</p></li></ul><h2>Topic: connecting a frontend to a matching backend</h2><p>Argo (domain identifier + port) tuple is needed for each end</p><ul><li><p>Note: the toolstack knows about both the front and backend and could<br/>be able to connect the two together.</p></li><li><p>Note: \"dom0less\" case is important: no control domain or toolstack<br/>executing concurrently to perform any connection handshake.<br/>DomB should be able to provide configuration information to each domain<br/>and have connections succeed as the drivers in the different domains<br/>connect to each other.</p><ul><li><p>domain reboot plan in that case: to be considered</p></li></ul></li><li><p>Note: options: frontend can:</p><ul><li><p>use Argo wildcards, or</p></li><li><p>know which domain+port to connect to the backend</p></li><li><p>or be preconfigured by some external entity to enable the connection<br/>(with Argo modified to enable this, per Cambridge plan)</p></li></ul></li><li><p>Note: ability to restart a backend (potentially to a different domid)<br/>is valuable</p></li><li><p>Note: Qemu is already used as a single-purpose backend driver for storage.</p><ul><li><p>ref: use of Qemu in Xen-on-ARM (PVH architecture, not HVM device emulator)</p></li><li><p>ref: Qemu presence in the xencommons init scripts</p></li><li><p>ref: qcow filesystem driver</p></li><li><p>ref: XenServer storage?</p></li></ul></li></ul><h2>Design topic: surfacing bootstrap/init connection info to guests</h2><p>Options:</p><ol><li><p>PCI device, with emulation provided by Xen</p></li><li><p>ACPI tables</p><ul><li><p>dynamic content ok: cite existing CPU hotplug behaviour</p></li><li><p>executable content ok: AML</p></li></ul></li><li><p>External connection of Argo ports, performed by a toolstack</p><ul><li><p>ref: Cambridge Argo design discussion, Q4 2019</p></li></ul></li><li><p>Use a wildcard ring on the frontend side for driver to autoconnect</p><ul><li><p>can scope reachability via the Argo firewall (XSM or nextgen impl)</p></li></ul></li></ol><p>Note:<br/>ACPI plan is good for compatibility with DomB work, where ACPI tables are<br/>already being populated to enable PVH guest launch.</p><h2>Plan:</h2><ul><li><p>ACPI table -&gt; indicates the Argo tuple(s) for a guest to register<br/>virtio-argo transport</p><ul><li><p>note: if multiple driver domains, will need (at least) one transport<br/>per driver domain</p></li></ul></li><li><p>toolstack passes the backend configuration info to the qemu (or demu) instance<br/>that is runnning the backend driver</p></li><li><p>qemu: has an added argo backend for virtio</p></li><li><p>the backend connects to the frontend to negociate transport</p></li></ul><p>For a first cut: just claim a predefined port and use that to avoid the need<br/>for interacting with an ACPI table.</p><h3>To build:</h3><ul><li><p>an early basic virtio transport over Argo as a proof of viability</p></li></ul><h3>Wiki page to include:</h3><ul><li><p>describe compatibility of the plan wrt to the Cambridge Argo design<br/>discussion which covered Argo handling communication across nested<br/>virtualization.</p></li><li><p>how could a new CPU VMFUNC assist Argo?</p><ul><li><p>aim: to obviate or mitigate the need for VMEXITs in the data path.</p></li></ul></li><li><p>look at v4v's careful use of interrupts (rather than events) for<br/>delivery of notifications: should be able reduce the number of Argo VMEXITS.<br/>-&gt; see the Bromium uxen v4v driver and the first-posted round of the<br/>Argo upstreaming series.</p></li></ul><h2>License of this Document</h2><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"31\" ac:original-width=\"88\" ac:width=\"102\"><ri:attachment ri:filename=\"cc-by-sa-88x31.png?version=1&amp;modificationDate=1463426853195&amp;cacheVersion=1&amp;api=v2\" ri:version-at-save=\"1\"></ri:attachment></ac:image><p>Copyright (c) 2020 BAE Systems.<br/>Document author: Christopher Clark.<br/>This work is licensed under the Creative Commons Attribution Share-Alike 4.0 International License.<br/>To view a copy of this license, visit <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">https://creativecommons.org/licenses/by-sa/4.0/</a>.</p>",
    "date": "2024-11-15",
    "disclaimer": "Users of this benchmark dataset are advised to check Atlassian’s official documentation for the most current information.",
    "space": "DC"
}