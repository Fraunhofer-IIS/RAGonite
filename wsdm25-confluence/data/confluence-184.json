{
    "id": "confluence-184",
    "title": "VM Memory Layout",
    "url": "https://openxt.atlassian.net/wiki/spaces/DC/pages/21987382/VM+Memory+Layout",
    "content": "<p>Owned by Ross Philipson\nLast updated: Feb 12, 2016 by Ross Philipson\n\n</p><ac:structured-macro ac:macro-id=\"9a231856-1a29-4ed0-adc8-bb13d699c701\" ac:name=\"info\" ac:schema-version=\"1\"><ac:parameter ac:name=\"icon\">None</ac:parameter><ac:rich-text-body><p><ac:image ac:width=\"80\"><ri:attachment ri:filename=\"cc-by.png\"></ri:attachment></ac:image></p><p>Copyright 2016 by Assured Information Security, Inc. Created by Ross Philipson &lt;philipsonr@<a href=\"http://ainfosec.com\">ainfosec.com</a>&gt;. This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit <a class=\"external-link\" href=\"http://creativecommons.org/licenses/by/4.0/\" rel=\"nofollow\">http://creativecommons.org/licenses/by/4.0/</a>.</p></ac:rich-text-body></ac:structured-macro><h2>Memory Model</h2><p>The rundown here will focus on HVMs, though PV guests are not all that different. The following is the memory layout from <code>xenops/memory.ml</code>:</p><ac:structured-macro ac:macro-id=\"65b79b78-d9f0-449a-9b28-35bedc175fce\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">text</ac:parameter><ac:parameter ac:name=\"title\">Memory Layout</ac:parameter><ac:plain-text-body><![CDATA[(* === Domain memory breakdown ============================================== *)\n\n(*           ╤  ╔══════════╗                                     ╤            *)\n(*           │  ║ shadow   ║                                     │            *)\n(*           │  ╠══════════╣                                     │            *)\n(*  overhead │  ║ extra    ║                                     │            *)\n(*           │  ║ external ║                                     │            *)\n(*           │  ╠══════════╣                          ╤          │            *)\n(*           │  ║ extra    ║                          │          │            *)\n(*           │  ║ internal ║                          │          │            *)\n(*           ╪  ╠══════════╣                ╤         │          │ footprint  *)\n(*           │  ║ video    ║                │         │          │            *)\n(*           │  ╠══════════╣  ╤    ╤        │ actual  │ xen      │            *)\n(*           │  ║          ║  │    │        │ /       │ maximum  │            *)\n(*           │  ║          ║  │    │        │ target  │          │            *)\n(*           │  ║ guest    ║  │    │ build  │ /       │          │            *)\n(*           │  ║          ║  │    │ start  │ total   │          │            *)\n(*    static │  ║          ║  │    │        │         │          │            *)\n(*   maximum │  ╟──────────╢  │    ╧        ╧         ╧          ╧            *)\n(*           │  ║          ║  │                                               *)\n(*           │  ║          ║  │                                               *)\n(*           │  ║ balloon  ║  │ build                                         *)\n(*           │  ║          ║  │ maximum                                       *)\n(*           │  ║          ║  │                                               *)\n(*           ╧  ╚══════════╝  ╧                                               *)]]></ac:plain-text-body></ac:structured-macro><p><span>The blocks marked build maximum and video are passed to xenvm via the input configuration. The balloon area is the memory available to be ballooned up using xenops balloon option when Populate on Demand (PoD) is used. The extra internal block is an extra Mb of total memory given to the guest.</span></p><p><span> </span>TODO: It is not totally clear what extra external and shadow are but they are external to the guest's memory. Presumably they are for use by xenvm.</p><p>The memory model is defined in <code>xenops/memory.ml</code>. The extra blocks and actual memory model structs are found here:</p><ac:structured-macro ac:macro-id=\"c4665e7c-c1c0-40a4-93b3-912d2fe4f219\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">c#</ac:parameter><ac:plain-text-body><![CDATA[module HVM_memory_model_data : MEMORY_MODEL_DATA = struct\n        let extra_internal_mib = 1L (* The extra 1Mb *)\n        let extra_external_mib = 1L\nend\n \n...\n \nmodule Memory_model (D : MEMORY_MODEL_DATA) = struct\n       (* memory block definitions generated here *)\n       ...\nend]]></ac:plain-text-body></ac:structured-macro><p><span>The VM building code mainly resides in <code>xenops/domain_control.ml</code>. Starting in <code>build_hvm</code> which is called out of <code>xenvm/vmact.ml</code>, all the memory values are fetched:</span></p><ul><li><code>static_max_mib</code> comes from <code>static_max_kib</code> passed to the function. It is the <code>build_max_mib</code> + <code>video_mib</code>.</li><li><code>xen_max_mib</code> is calculated in the memory model as <span style=\"line-height: 1.42857;\"><code>static_max_mib</code> + <code>D.extra_internal_mib</code></span></li><li><span style=\"line-height: 1.42857;\"><code>build_max_mib </code>and<code> build_start_mib</code> end up being the same for HVMs in OpenXT because <code>static_max_mib</code> and <code>target_mib</code> are the same. They are calculated in the memory model by subtracting <code>video_mib</code>.</span></li><li><span style=\"line-height: 1.42857;\"><code>build_max_mib </code>and<code> build_start_mib</code> may be different for guests when PoD mode is desired.</span></li></ul><p><span style=\"line-height: 20.0px;\">With the values above, the first stop is set the overall maximum memory for the guest. This is done in <code>build_pre</code> using <span><code>xen_max_mib</code>. After that step <code>build_hvm</code> then calls<code> Xg.hvm_build</code> (which eventually calls <code>libxc:xc_hvm_build</code>). The <code>build_start_mib</code> and <code>build_max_mib</code> values are passed and are used to physmap the initial memory for the guest. Note this does not include the <code>video_mid</code> or <span><code>D.extra_internal_mib</code>. More on those later. This is the relevant code, annotated:</span></span></span></p><ac:structured-macro ac:macro-id=\"813e1044-b92c-4850-92c9-276ced5f3255\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">text</ac:parameter><ac:plain-text-body><![CDATA[let build_pre ~xc ~xs ~vcpus ~xen_max_mib ~shadow_mib ~required_host_free_mib domid =\n        ...\n        (* This is the call to set the overall maximum for the guest. Note that none of this memory\n           is mapped yet. This is the the maximum possible memory *)\n        Xc.domain_setmaxmem xc domid (Memory.kib_of_mib xen_max_mib);\n        ...\n \nlet build_hvm ~xc ~xs ~static_max_kib ~target_kib ~video_mib ~shadow_multiplier ~vcpus\n              ~kernel ~timeoffset ~xci_cpuid_signature domid =\n        ...\n        (* Call build_pre first to set overall memory xen_max_mib *)\n        let store_port, console_port = build_pre ~xc ~xs\n                ~xen_max_mib ~shadow_mib ~required_host_free_mib ~vcpus domid in\n \n        ...\n        (* Call Xg xenguest helper library to do the actual domain building via libc. The\n           build_start_mib and build_max_mib values are used to do the physmapping at this tine. *)\n        let store_mfn, console_mfn = Xg.hvm_build xgh domid (Int64.to_int build_max_mib) (Int64.to_int build_start_mib) kernel platformflags store_port console_port in\n ]]></ac:plain-text-body></ac:structured-macro><h3>Build start and max memory</h3><p>A note on the memory values used in <code>Xg.hvm_build</code>. In the actual call to <code>libxc</code>, the values are mapped as such:</p><ac:structured-macro ac:macro-id=\"cf141fdc-12ff-4112-9583-addee8ebb20d\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">cpp</ac:parameter><ac:plain-text-body><![CDATA[CAMLprim value stub_xc_hvm_build_native(value xc_handle, value domid,\n    value mem_max_mib, value mem_start_mib, value image_name, value platformflags, value store_evtchn, value console_evtchn)\n \n        ...\n        args.mem_size = (uint64_t) Int_val(mem_max_mib) << 20;\n        args.mem_target = (uint64_t) Int_val(mem_start_mib) << 20;\n \n        ...\n        /* In the libxc code, if mem_target < mem_size, Populate on Demand mode is set for the VM and\n         * during the physmap process, less that mem_size will get mapped initially. */\n        r = xc_hvm_build(xch, _D(domid), &args);]]></ac:plain-text-body></ac:structured-macro><h3>Video and extra Internal memory</h3><p>As noted above, the overall maximum amount of memory a guest is allowed includes these values but they are not physmapped in by the domain builder code. They are in fact physmapped by QEMU. In OpenXT the video memory is 16Mb. That is mapped in <code>xen-all.c:xen_ram_alloc</code>. Any additional devices like xenmou can also physmap more memory in the extra internal region with as noted is 1M.</p><h2>xenvm vs. libxl</h2><p>The good news is that all of the above machinery is more or less the same for <code>libxl</code>:</p><ac:structured-macro ac:macro-id=\"cd1a45a6-2ab7-4be5-9054-5bd32dd4bded\" ac:name=\"code\" ac:schema-version=\"1\"><ac:parameter ac:name=\"language\">cpp</ac:parameter><ac:plain-text-body><![CDATA[/* Defined in libxl_internal.h, this is the extra internal memory in Kb */\n#define LIBXL_MAXMEM_CONSTANT 1024\n \nint libxl__build_pre(libxl__gc *gc, uint32_t domid,\n              libxl_domain_config *d_config, libxl__domain_build_state *state)\n    ...\n    /* Target memory like max memory in the info struct includes the video memory. That has\n     * the extra internal memory added during the call to set the overall max memory for the\n     * guest. Note the values are in Kb in this case. */\n    xc_domain_setmaxmem(ctx->xch, domid, info->target_memkb + LIBXL_MAXMEM_CONSTANT);\n    ...\n \nint libxl__build_hvm(libxl__gc *gc, uint32_t domid,\n              libxl_domain_build_info *info,\n              libxl__domain_build_state *state)\n    ...\n    /* As before, the video memory is removed from the mem_size and mem_target before\n     * being sent to the libxc domain build to get physmapped. The shift 10 is due to some\n     * weirdness in the values in libxl, see the comment in the code. */\n    args.mem_size = (uint64_t)(info->max_memkb - info->video_memkb) << 10;\n    args.mem_target = (uint64_t)(info->target_memkb - info->video_memkb) << 10;\n    ...\n    ret = xc_hvm_build(ctx->xch, domid, &args);]]></ac:plain-text-body></ac:structured-macro><p>TODO: xenvm uses the max memory value when doing the <code>xc_domain_setmaxmem</code> call where libxl uses the target memory which can be smaller when using PoD. Needs more investigation but most likely handled during the ballooning process. That probably means xenvm is really doing it wrong.</p>",
    "date": "2024-11-15",
    "disclaimer": "Users of this benchmark dataset are advised to check Atlassian’s official documentation for the most current information.",
    "space": "DC"
}