{
    "id": "confluence-052",
    "title": "VirtIO & Argo",
    "url": "https://openxt.atlassian.net/wiki/spaces/DC/pages/1348763698/VirtIO+&+Argo",
    "content": "<p>Owned by Christopher Clark\nLast updated: Dec 11, 2020 by Christopher Clark\n\n</p><p>This document describes technical context and analysis to support a proposal for development of a new Linux device driver destined for mainline Linux and use by OpenXT, to introduce Hypervisor-Mediated data eXchange (HMX) into the data transport of the popular VirtIO suite of Linux virtual device drivers, by leveraging Argo in Xen. Daniel Smith proposed this idea, which has been supported in discussions by Christopher Clark and Rich Persaud of MBO/BAE Systems, and Eric Chanudet and Nick Krasnoff of AIS. Christopher is the primary author of this version of this document.</p><p>– August 2020</p><h1>Introduction to VirtIO</h1><p>VirtIO is a virtual device driver standard developed originally for the Linux kernel, drawing upon the lessons learned during the development of paravirtualized device drivers for Xen, KVM and other hypervisors. It aimed to become a “de-facto standard for virtual I/O devices”, and to some extent has succeeded in doing so. VirtIO is now widely implemented in both software and hardware, it is commonly the first choice for virtual driver implementation in new virtualization technologies, and the specification is now maintained under governance of the OASIS open standards organization.</p><p>VirtIO’s system architecture abstracts device-specific and device-class-specific interfaces and functionality from the transport mechanisms that move data and issue notifications within the kernel and across virtual machine boundaries. It is attractive to developers seeking to implement new drivers for a virtual device because VirtIO provides documented specified interfaces with a well-designed, efficient and maintained common core implementation that can be leveraged to significantly reduce the amount of work required for a new virtual device driver.</p><p>VirtIO follows the Xen PV driver model of split-device drivers, where a front-end device driver runs within the guest virtual machine to provide the device abstraction to the guest kernel, and a back-end driver runs outside the VM, in platform-provided software - eg. within a QEMU device emulator - to communicate with the front-end driver and provide mediated access to physical device resources.</p><p>A critical property of the current common VirtIO implementations is that they prevent enforcement of strong isolation between the front-end and back-end virtual machines, since the back-end VirtIO device driver is required to be able to obtain direct access to the memory owned by the virtual machine running the front-end virtio device driver. ie. The VM hosting the back-end driver has significant privilege over any VM running a front-end driver.</p><p>Xen’s PV drivers use the grant-table mechanism to confine shared memory access to specific memory pages used and permission to access those are specifically granted by the driver in the VM that owns the memory. Argo goes further and achieves stronger isolation than this since it requires no memory sharing between communicating virtual machines. </p><p>In contrast to Xen’s current driver transport options, the current implementations of virtio transports pass memory addresses directly across the VM boundary, under the assumption of shared memory access, and thereby require the back-end to have sufficient privilege to directly access any memory that the front-end driver refers to. This has presented a challenge for the suitability of using virtio drivers for Xen deployments where isolation is a requirement. Fortunately, a path exists for integration of the Argo transport into VirtIO which can address this and enable use of the existing body of virtio device drivers with isolation maintained and mandatory access control enforced: consequently this system architecture is significantly differentiated from other options for virtual devices.</p><h2>VirtIO Architecture Overview</h2><p>In addition to the front-end / back-end split device driver model, there are further standard elements of VirtIO system architecture.</p><p style=\"margin-left: 30.0px;\"><em>For detailed reference, VirtIO is described in detail in the “VirtIO 1.1 specification” OASIS standards document.</em></p><p>The front-end device driver architecture imposes tighter constraints on implementation direction, and so is more important to understand in detail, since it is this that is already implemented in the wide body of existing VirtIO device drivers that we are aiming to enable use of. The back-end software is implemented in the platform-provided software - ie. the hypervisor, toolstack, a platform-provided VM or a device emulator, etc. - where we have more flexibility in implementation options, and the interface is determined by both the host virtualization platform and the new transport driver that we are intending to create.</p><h3>VirtIO front-end driver classes</h3><p>There are multiple classes of virtio device driver within the Linux kernel; these include the general class of <em>front-end virtio device drivers</em>, which provide function-specific logic to implement virtual devices - eg. a virtual block device driver for storage - and the <em>transport virtio device drivers</em>, which are responsible for device discovery with the platform and provision of data transport across the VM boundary between the front-end drivers and the corresponding remote back-end driver running outside the virtual machine.</p><h3>VirtIO transport drivers</h3><p>There are several implementations of virtio transport device drivers in Linux, each implement a common interface within the kernel, and they are designed to be interchangeable and compatible with the VirtIO front-end drivers: so the same front-end driver can use different transports on different systems. Transports can coexist: different virtual devices can be using different transports within the same virtual machine at the same time.</p><h3>Access to virtual device buffers for I/O operations with VirtIO-PCI transport</h3><p>The back-end domain requires sufficient privilege with the hypervisor to be able to map the memory of any buffers used for I/O with the device by the guest VM.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"983\" ac:original-width=\"1108\"><ri:attachment ri:filename=\"device-buffer-access-virtio.png\" ri:version-at-save=\"2\"></ri:attachment></ac:image><h2>VirtIO with Argo transport</h2><p>Enabling Virtio to use the Argo interdomain communication mechanism for data transport across the VM boundary will address three critical requirements:</p><ul><li><p>Preserve strong isolation between the two ends of the split device driver</p><ul><li><p>ie. remove the need for any shared memory between domains or any privilege to map the memory belonging to the other domain</p></li></ul></li><li><p>Enable enforcement of granular mandatory access control policy over the communicating endpoints</p><ul><li><p>ie. Use Xen’s XSM/Flask existing control over Argo communication, and leverage new Argo MAC capabilities as they are introduced to govern Virtio devices </p></li></ul></li><li><p>Enable Argo support in the mainline Linux kernel</p><ul><li><p>The VirtIO-Argo transport device driver will be a smaller, simpler device driver than the existing general Argo Linux driver. It should not need to expose functionality to userspace directly, and should be simpler to develop for inclusion in the mainline Linux kernel, while still enabling Argo to be used for driver front-ends.</p></li><li><p>ie. It avoids many challenges with design, implementation and upstreaming of the existing Argo Linux device driver into the Linux kernel.</p></li></ul></li></ul><p>The proposal is to implement <em>a new VirtIO transport driver for Linux that utilizes Argo.</em> It will be used within guest virtual machines, and be compatible with the existing VirtIO front-end device drivers. It will be paired with a corresponding new VirtIO-Argo back-end to run within the Qemu device emulator, in the same fashion as the existing VirtIO transport back-ends, and the back-end will use libargo and the (non-VirtIO) Argo Linux driver.</p><h2>Using VirtIO</h2><h3>Front-end, in-guest software</h3><p>VirtIO device drivers are included in the mainline Linux kernel and enabled in most modern Linux distributions. There is a menu for VirtIO drivers in the kernel Kconfig to enable inclusion of drivers as required. Once the VirtIO-Argo transport driver has been reviewed upstream and accepted into the mainline Linux kernel, it should propagate for inclusion in the Linux distributions, which will enable seamless deployment of guest VMs on VirtIO-Argo hypervisor platforms with no futher in-guest drivers required.</p><p>Prior to the VirtIO-Argo device driver being made available via the Linux distributions, installation will require a Linux kernel module to be installed for the VirtIO-Argo driver, which will then enable compatibility with the other existing VirtIO device drivers in the guest. A method of switching devices over from using their prior driver to the newly activated VirtIO-Argo driver will need to be designed; this is the same issue that the existing Xen PV drivers have handled when handing off responsibility from running on emulated devices over to the Xen PV driver.</p><p>Open Source VirtIO drivers for Windows are available, with some Linux distributions, eg. Ubuntu and Fedora, including WHQL Certified drivers. These enable Windows guest VMs to run with virtual devices provided by the VirtIO backends in QEMU. It has not yet been ascertained whether the Windows VirtIO implementation is suitable for introduction of a VirtIO-Argo transport driver in the same way as proposed here for Linux.</p><h3>Host platform software</h3><h4>QEMU</h4><p>The QEMU device emulator implements the Virtio transport that the front-end will connect to. Current QEMU 5.0 implements both the virtio-pci and virtio-mmio common transports.</p><h4>Linux Argo driver</h4><p>For QEMU to be able to use Argo, it will need an Argo Linux kernel device driver, with similar functionality to the existing Argo Linux driver.</p><h4>Toolstack</h4><p>The toolstack of the hypervisor is responsible for configuring and establishing the back-end devices according to the virtual machine configuration. It will need to be aware of the VirtIO-Argo transport and initialize the back-ends for each VM with a suitable configuration for it.</p><h4>Optional: Userspace drivers</h4><p>The DPDK userspace device driver software also implements an alternative set of VirtIO virtual device back-ends to QEMU, which can also be used to support VMs with VirtIO virtual devices. Please note that the DPDK implementation uses a “packed virtqueue” data structure, as opposite to the default original “split virtqueue”, across the VM boundary and this is not targetted for compatibility in the initial version of the VirtIO-Argo transport.</p><h3>VirtIO hardware</h3><p>Hardware implementations of VirtIO interfaces will by unaffected by the VirtIO-Argo system.</p><h2>Using the existing Argo and Xen software in OpenXT: without VirtIO</h2><p>OpenXT currently uses the Xen PV drivers for virtual devices.</p><p>The Xen PV drivers use the Grant Tables for the front-end domain to grant access to the back-end domain to be able to establish shared memory access to specific buffers for physical I/O device operations.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"989\" ac:original-width=\"1107\"><ri:attachment ri:filename=\"device-buffer-access-pv-drivers.png\" ri:version-at-save=\"2\"></ri:attachment></ac:image><p>In addition to the Xen PV drivers, OpenXT also uses Argo, with an Argo Linux device driver, for inter-domain communication. In contrast to the grant-table data path used by the Xen PV drivers, which establishes and tears down shared memory regions between communicating VMs, the Argo data path is hypervisor-mediated, with Mandatory Access Control enforced on every data movement operation.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"979\" ac:original-width=\"1107\"><ri:attachment ri:filename=\"device-buffer-access-argo.png\" ri:version-at-save=\"1\"></ri:attachment></ac:image><p>The challenges with using Argo with its own Linux device driver installed in the guest are:</p><ul><li><p>The Argo Linux driver is not currently suitable for upstreaming to mainline Linux.</p><ul><li><p>A development plan exists for improvement but it requires significant effort and resourcing it has proven challenging.</p></li></ul></li><li><p>To produce an Argo-transport driver of each virtual device driver class, such as networking, storage, etc., suitable for replacing the existing Xen PV drivers, is work that would be specific to only the Xen software ecosystem.</p><ul><li><p>Not all Xen-based projects would be likely to switch virtual driver implementation, given the different characteristics and requirements of cloud platforms, client platforms and embedded systems, and the existing investment in the Xen PV drivers software</p><ul><li><p>eg. see the recent automotive community development of PV audio for in-vehicle entertainment systems.</p></li></ul></li><li><p>This stands in contrast to the VirtIO network and storage virtual device drivers that can be used unchanged on a number of different hypervisors.</p></li></ul></li></ul><h2>Using VirtIO with Argo in OpenXT</h2><p>Adding Argo as a transport for VirtIO will retain Argo’s MAC policy checks on all data movement, while allowing use of the VirtIO virtual device drivers and device implementations.</p><ac:image ac:align=\"center\" ac:layout=\"center\" ac:original-height=\"1241\" ac:original-width=\"1106\"><ri:attachment ri:filename=\"device-buffer-access-virtio-argo.png\" ri:version-at-save=\"1\"></ri:attachment></ac:image><p>With the VirtIO virtual device drivers using the VirtIO-Argo transport driver, OpenXT can retire use of the Xen PV drivers within platform VMs and guest workloads. This removes shared memory from the data path of the device drivers, allows for some hypervisor functionality, such as the grant tables, to be disabled, and makes the virtual device driver data path <a href=\"https://www.platformsecuritysummit.com/2018/speaker/clark/\">HMX-compliant</a>.</p><p>In addition, as new virtual device classes in Linux have VirtIO drivers implemented, these should transparently be enabled with Mandatory Access Control, via the existing virtio-argo transport driver, potentially without further effort required – although please note that for some cases (eg. graphics) optimizing performance characteristics may require additional effort.</p><h2>VirtIO with Argo: architecture and design</h2><p></p><h2>TODO:</h2><ul><li><p>Design of the ACPI-table-based virtual device discovery mechanism for the VirtIO-Argo transport</p></li><li><p>Research and design of the Argo-virtio backend transport architecture</p><ul><li><p>Toolstack</p></li><li><p>QEMU</p></li><li><p>Review XSM/Flask policy control points in Argo for the split-driver use case</p></li><li><p>xenstore interaction, if any</p></li></ul></li></ul><h2>Needed</h2><ul><li><p>An Argo specification document</p><ul><li><p>Required for supporting embedded system evaluations</p><ul><li><p>Repeatedly requested by interested parties in the Xen ecosystem who work with safety requirements.</p></li></ul></li><li><p>This will be appropriate for inclusion in the Xen Project documentation</p></li></ul></li><li><p>A performance evaluation of Argo</p><ul><li><p>Report required to support external evaluations of Argo</p><ul><li><p>Question has been raised in every forum where Argo has been presented</p></li></ul></li><li><p>Needs to cover Intel, AMD and Arm platforms</p><ul><li><p>Variation in processor cache architectures and memory consistency models likely to have appreciable effects</p></li></ul></li><li><p>Development to improve performance based on measurements</p><ul><li><p>Notification delivery is a known area where improvement should be possible</p><ul><li><p>See the first series of Argo posted during Xen upstreaming for a non-event based interrupt delivery path</p></li></ul></li></ul></li><li><p>Split-driver use case is a priority</p><ul><li><p>bandwidth important for bulk data transport</p></li><li><p>latency important for interactive and media-delivery (eg. IVI systems)</p></li></ul></li></ul></li></ul><h2>References</h2><ul><li><p>The VirtIO 1.1 specification, OASIS<br/><a href=\"https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html\">https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html</a></p></li><li><p>virtio: Towards a De-Facto Standard For Virtual I/O Devices<br/>Rusty Russell, IBM OzLabs; ACM SIGOPS Operating Systems Review, 2008.<br/><a href=\"https://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf\">https://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf</a></p></li><li><p>Xen and the Art of Virtualization<br/>Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, Andrew Warfield; ACM Symposium on Operating System Principles, 2003<br/><a href=\"https://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf\">https://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf</a></p></li><li><p>Project ACRN: 1.0 chose to implement virtio<br/><a href=\"https://projectacrn.org/acrn-project-releases-version-1-0/\">https://projectacrn.org/acrn-project-releases-version-1-0/</a></p></li><li><p>Solo5 unikernel runtime: chose to implement virtio<br/><a href=\"https://mirage.io/blog/introducing-solo5\">https://mirage.io/blog/introducing-solo5</a></p></li><li><p>Windows VirtIO drivers<br/><a href=\"https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers\">https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers</a><br/><a data-card-appearance=\"inline\" href=\"https://github.com/virtio-win/kvm-guest-drivers-windows\">https://github.com/virtio-win/kvm-guest-drivers-windows</a> </p></li></ul><h1>Notes from Virtio Argo planning meeting, July 2020</h1><h2>Call on Friday 17th July</h2><p>Attendees:</p><ul><li><p>Christopher Clark, Rich Persaud; MBO/BAE</p></li><li><p>Daniel Smith; Apertus Solutions</p></li><li><p>Eric Chanudet, Nick Krasnoff;  AIS Boston</p></li></ul><p>Actions:</p><ul><li><p>Christopher to produce a draft plan, post to Wire, discuss and then<br/>put it onto a private page on the OpenXT confluence wiki</p></li><li><p>Christopher and Eric to commence building a prototype as funding permits,<br/>with Daniel providing design input and review</p></li></ul><h2>Topic: Argo as a transport for Virtio</h2><p>Background: the Virtio implementation in the Linux kernel is stacked:</p><ul><li><p>there are many virtio drivers for different virtual device functions</p></li><li><p>each virtio driver uses a common in-kernel virtio transport API,<br/>and there are multiple alternative transport implementations</p></li><li><p>transport negociation proceeds, then driver-level negociation<br/>to establish the frontend-backend communication for each driver</p></li></ul><p>KVM uses Qemu to provide backend implementations for devices.<br/>To implement Argo as a Virtio transport, will need to make Qemu Argo-aware.</p><p>Typically a PCI device is emulated by Qemu, on a bus provided to the guest.<br/>The Bus:Device:Function (bdf) triple maps to Qemu; the guest loads the PCI<br/>virio transport when this PCI device with that bdf is detected.</p><p>For Xen, this is OK for HVM guests, but since the platform PCI device is not<br/>exposed to PVH (or PV) guests, not a sufficient method for those cases.</p><h3>Discussion notes</h3><p>The existing main Xen PCI code is small, used for discovery.<br/>vPCI is experimental full emulation of PCI within Xen -- Daniel does not<br/>favour this direction.</p><p>Paul Durrant has a draft VFIO implementation plan, previously circulated.<br/>Paul is working on emulating PCI devices directly via a IOReq server (see: \"<a href=\"https://xenbits.xen.org/gitweb/?p=people/pauldu/demu.git;a=tree\">demu</a>\").</p><p>Daniel is interested in the Linux kernel packet mmap, to avoid one world-switch<br/>when processing data.</p><p>Christopher raised the Cambridge Argo port-connection design for external<br/>connection of Argo ports between VMs.</p><p>Eric raised the vsock-argo implementation; discussion mentioned the Argo development wiki page that describes requirements and plan that has previously been discussed and agreed for future Argo Linux driver development, which needs to include a non-vsock interface for administrative actions such as runtime firewall configuration.</p><p>Daniel is positively inclined towards using ACPI tables for discovery<br/>and notes the executable ACPI table feature, via AML.</p><p>Naming: an Argo transport for virtio in the Linux kernel would, to match the<br/>existing naming scheme, be: virtio-argo.c</p><p>Group inclined not to require XenStore.</p><ul><li><p>Relevant to dom0less systems where XenStore is not necessary present.</p></li><li><p>Don't want to introduce another dependency on it.</p></li><li><p>uXen demonstrates that it is not a requirement.</p></li><li><p>XenStore is a performance bottleneck (see NoXS work), bad interface, etc.</p></li></ul><h2>Topic: connecting a frontend to a matching backend</h2><p>Argo (domain identifier + port) tuple is needed for each end</p><ul><li><p>Note: the toolstack knows about both the front and backend and could<br/>be able to connect the two together.</p></li><li><p>Note: \"dom0less\" case is important: no control domain or toolstack<br/>executing concurrently to perform any connection handshake.<br/>DomB should be able to provide configuration information to each domain<br/>and have connections succeed as the drivers in the different domains<br/>connect to each other.</p><ul><li><p>domain reboot plan in that case: to be considered</p></li></ul></li><li><p>Note: options: frontend can:</p><ul><li><p>use Argo wildcards, or</p></li><li><p>know which domain+port to connect to the backend</p></li><li><p>or be preconfigured by some external entity to enable the connection<br/>(with Argo modified to enable this, per Cambridge plan)</p></li></ul></li><li><p>Note: ability to restart a backend (potentially to a different domid)<br/>is valuable</p></li><li><p>Note: Qemu is already used as a single-purpose backend driver for storage.</p><ul><li><p>ref: use of Qemu in Xen-on-ARM (PVH architecture, not HVM device emulator)</p></li><li><p>ref: Qemu presence in the xencommons init scripts</p></li><li><p>ref: qcow filesystem driver</p></li><li><p>ref: XenServer storage?</p></li></ul></li></ul><h2>Design topic: surfacing bootstrap/init connection info to guests</h2><p>Options:</p><ol><li><p>PCI device, with emulation provided by Xen</p></li><li><p>ACPI tables</p><ul><li><p>dynamic content ok: cite existing CPU hotplug behaviour</p></li><li><p>executable content ok: AML</p></li></ul></li><li><p>External connection of Argo ports, performed by a toolstack</p><ul><li><p>ref: Cambridge Argo design discussion, Q4 2019</p></li></ul></li><li><p>Use a wildcard ring on the frontend side for driver to autoconnect</p><ul><li><p>can scope reachability via the Argo firewall (XSM or nextgen impl)</p></li></ul></li></ol><p>Note:<br/>ACPI plan is good for compatibility with DomB work, where ACPI tables are<br/>already being populated to enable PVH guest launch.</p><h2>Plan:</h2><ul><li><p>ACPI table -&gt; indicates the Argo tuple(s) for a guest to register<br/>virtio-argo transport</p><ul><li><p>note: if multiple driver domains, will need (at least) one transport<br/>per driver domain</p></li></ul></li><li><p>toolstack passes the backend configuration info to the qemu (or demu) instance<br/>that is runnning the backend driver</p></li><li><p>qemu: has an added argo backend for virtio</p></li><li><p>the backend connects to the frontend to negociate transport</p></li></ul><p>For a first cut: just claim a predefined port and use that to avoid the need<br/>for interacting with an ACPI table.</p><h3>To build:</h3><ul><li><p>an early basic virtio transport over Argo as a proof of viability</p></li></ul><h3>Wiki page to include:</h3><ul><li><p>describe compatibility of the plan wrt to the Cambridge Argo design<br/>discussion which covered Argo handling communication across nested<br/>virtualization.</p></li><li><p>how could a new CPU VMFUNC assist Argo?</p><ul><li><p>aim: to obviate or mitigate the need for VMEXITs in the data path.</p></li></ul></li><li><p>look at v4v's careful use of interrupts (rather than events) for<br/>delivery of notifications: should be able reduce the number of Argo VMEXITS.<br/>-&gt; see the Bromium uxen v4v driver and the first-posted round of the<br/>Argo upstreaming series.</p></li></ul><h2>License of this Document</h2><ac:image ac:align=\"left\" ac:layout=\"align-start\" ac:original-height=\"31\" ac:original-width=\"88\" ac:width=\"102\"><ri:attachment ri:filename=\"cc-by-sa-88x31.png?version=1&amp;modificationDate=1463426853195&amp;cacheVersion=1&amp;api=v2\" ri:version-at-save=\"1\"></ri:attachment></ac:image><p>Copyright (c) 2020 BAE Systems.<br/>Document author: Christopher Clark.<br/>This work is licensed under the Creative Commons Attribution Share-Alike 4.0 International License.<br/>To view a copy of this license, visit <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">https://creativecommons.org/licenses/by-sa/4.0/</a>.</p>",
    "date": "2024-11-15",
    "disclaimer": "Users of this benchmark dataset are advised to check Atlassian’s official documentation for the most current information.",
    "space": "DC"
}