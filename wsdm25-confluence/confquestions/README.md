This directory contains the ConfQuestions benchmark, that can be used for evaluating RAG-based conversational question answering. The questions, answers, and other metadata can be found in qa-pairs.json. The document collection consists of JSON files confluence-001.json through confluence-215.json, each file corresponding to one of the 215 Confluence pages used in our study, and is inside the documents/ directory. The LLM-as-a-judge evaluation prompt (we used GPT-4o) is in the eval folder, along with a skeleton of our original evaluation script. ConfQuestions statistics are in stats.pdf.
