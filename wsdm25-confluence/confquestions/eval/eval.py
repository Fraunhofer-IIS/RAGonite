import dataclasses
import json
import shutil
import time
from typing import List, Optional, Literal, Dict
from dataclasses import dataclass, field
from pathlib import Path
import asyncio
import re

import pandas as pd
from pydantic import TypeAdapter


"""
DISCLAIMER:

Please note that this evaluation script is integrated into our proprietary RAG system RAGonite. 
Therefore, some of the modules used in this script are not included in this public codebase. 

However, for the evaluation results of this work, we disabled any advanced features our RAG system provides.
Thus, our evaluation results can be replicated by feeding the output of `src/prepare.py` into a ChromaDB 
and by plugging in an arbitrary RAG system that supports history as the backbone of this script. 

You can find the prompt used for the RAG backbone in prompt.txt of this folder.
"""


ANSWER_CHECK_PROMPT_TEMPLATE = """
You are a helpful assistant to evaluate the answer generated by an LLM in a retrieval-augmented generation (RAG) setup against a human-generated answer. Please score the generated answer with respect to the human-generated answer with:

- (a) 1 if the generated answer contains all information in the human answer (completely relevant);
- (b) 0.5  if the generated answer contains partial information with respect to the human answer (partially relevant);
- (c) 0 if the generated answer contains no overlap in information with respect to the human answer (non-relevant).

Please note that in all cases, exact string matches are not required: please check for semantic equivalence.
Do not rate with True or False. Always give a score as instructed above. Always provide a justification for your rating. 
Please provide your response in valid JSON.

Use the following three templates as a reference:

{{
  "answer_rating": 1,
  "justification": "The generated answer contains all the necessary information from the ground truth answer, including details about X and Y."
}}

{{
  "answer_rating": 0.5,
  "justification": "The generated answer contains some of the information from the ground truth answer, however, information A nd B is missing."
}}

{{
  "answer_rating": 0,
  "justification": "The generated answer contains no overlap in information with respect to the ground truth answer"
}}

Now let's start:

**Question:** {question}

**Given Answer:** {answer}

**Ground Truth Answer:** {ground_truth_answer}
"""


def strip_markdown_code_block(answer_evaluation: str) -> str:
    """
    More often than not, GPT-4o will return structured formated JSON responses wrapped in markdown, i.e.:
    ```json
    ...
    ```

    It can be improved by prompt engineering, but let's make sure we have valid JSON.

    :param answer_evaluation: json result string from LLM
    :return: result string without markdown.
    """
    pattern = r"```(?:json)?\s*([\s\S]*?)```"

    match = re.search(pattern, answer_evaluation)
    if match:
        answer_evaluation = match.group(1).strip()

    return answer_evaluation


@dataclass
class Turn:
    turn_id: str
    q_type: str
    q_en: str
    q_de: str
    completed_q_en: str
    completed_q_de: str
    a_url: List[str]
    a_source: str
    a: str

@dataclass
class Conversation:
    conv_id: str
    turns: List[Turn] = field(default_factory=list)
    history: Dict[str, List[RAGMessage]] = field(default_factory=dict)


class BenchmarkDocumentRetrievalResult:
    query: str
    rephraser: str
    result: List[Dict[str, str]]
    recall: float
    precision: float
    correct_at_rank_1: bool


@dataclass
class BenchmarkConversationResult:
    question: str
    a_gen: str
    a: str
    answer_right: bool
    conv_id: str
    turn_id: str
    a_source: str
    q_type: str
    language_code: str
    answer_justification: Optional[str] = None
    document_retrieval: Optional[BenchmarkDocumentRetrievalResult] = None


def print_summary(conv, turn, question, answer, answer_justification, answer_rating, language_code, document_query, rephraser_mode):
    separator = "=" * 50
    sub_separator = "-" * 50

    print(separator)
    print(f"{'Conversation':<20}: {conv.conv_id}")
    print(f"{'Turn':<20}: {turn.turn_id}")
    print(f"{'Question Type':<20}: {turn.q_type}")
    print(f"{'Language ':<20}: {language_code}")
    print(f"{'Rephraser ':<20}: {rephraser_mode}")
    print(sub_separator)

    print(f"{'Question':<20}: {question}")
    print(f"{'Doc Query':<20}: {document_query}")
    print(f"{'Given Answer':<20}: {answer}")
    print(f"{'Ground Truth':<20}: {turn.a}")
    print(f"{'Answer Justification':<20}: {answer_justification}")
    print(f"{'Answer Rating':<20}: {answer_rating}")
    print(separator)
    print()


def contains_document_url(retrieved_documents: List[Document], ground_truth_documents: List[str]) -> dict:
    """
    Calculate precision, recall, and whether the correct URL is at the first position.

    Args:
    - retrieved_documents (List[Document]): A list of Document objects representing retrieved documents.
    - ground_truth_documents (List[str]): A list of string URLs representing ground truth documents.

    Returns:
    - dict: A dictionary containing the following keys:
        - 'recall' (float): The fraction of ground truth documents found in the retrieved documents.
        - 'precision' (float): The fraction of retrieved documents that are in the ground truth.
        - 'correct_at_rank_1' (bool): True if the first retrieved document is in the ground truth, False otherwise.
    """

    retrieved_urls = {doc.url for doc in retrieved_documents}
    ground_truth_set = set(ground_truth_documents)

    true_positives = len(ground_truth_set & retrieved_urls)
    precision = true_positives / len(retrieved_urls) if retrieved_urls else 0.0
    recall = true_positives / len(ground_truth_set) if ground_truth_set else 0.0
    correct_at_rank_1 = retrieved_documents[0].url in ground_truth_set if retrieved_documents else False

    return {
        'recall': recall,
        'precision': precision,
        'correct_at_rank_1': correct_at_rank_1
    }


async def run_conversation_benchmark(conv: Conversation, rag: RAG, language_codes: Optional[List[str]] = None, rephrased_from_input: bool = False, delay: float = 1.5,) -> List[BenchmarkConversationResult]:
    conversation_benchmark_results = []
    for turn in conv.turns:
        language_codes = language_codes or ["en"]
        for lang_code in language_codes:
            if lang_code not in conv.history:
                conv.history[lang_code] = []

        for lang_code in language_codes:
            answer = ""
            answer_evaluation = ""
            answer_rating = False
            answer_justification = ""
            document_query = None
            document_query_result = None

            question_field = f"q_{lang_code}"
            question = getattr(turn, question_field, None)

            if question is None:
                print(f"No question found for language code '{lang_code}'. Skipping...")
                continue

            if rephrased_from_input:
                rephrased_field = f"completed_q_{lang_code}"
                search_query = getattr(turn, rephrased_field, None)
            else:
                search_query = None

            async for event in generate_answer(question, conv.history[lang_code], rag, search_query, streaming=False):
                if isinstance(event, AnswerChunkEvent):
                    answer += event.chunk
                elif isinstance(event, DocumentQueryEvent):
                    document_query = event.query
                    document_query_result = event.documents
                time.sleep(delay)

            if document_query_result is not None and answer:
                prompt = ANSWER_CHECK_PROMPT_TEMPLATE.format(
                    ground_truth_answer=turn.a,
                    question=question,
                    answer=answer
                )

                try:
                    answer_evaluation = await rag.answer_generation_model.complete_prompt(prompt)
                    answer_evaluation = strip_markdown_code_block(answer_evaluation)

                    if answer_evaluation.strip():
                        try:
                            evaluation_data = json.loads(answer_evaluation)
                            answer_rating = evaluation_data.get("answer_rating", "0")
                            answer_justification = str(
                                evaluation_data.get("justification", ""))
                        except json.JSONDecodeError as e:
                            print(f"JSON decoding error: {e}")
                    else:
                        print("Received an empty or whitespace response from the model.")

                except Exception as e:
                    print(f"Unexpected error during model evaluation: {e}")
                document_retrieval_scores = contains_document_url(document_query_result, turn.a_url)

                document_results = []
                for doc in document_query_result:
                    document_results.append(
                        {"id": doc.id, "url": doc.url, "text": doc.content}
                    )

                conversation_benchmark_results.append(
                    BenchmarkConversationResult(
                            conv_id=conv.conv_id,
                            turn_id=turn.turn_id,
                            question=question,
                            a_gen=answer,
                            a=turn.a,
                            answer_right=answer_rating,
                            a_source=turn.a_source,
                            q_type=turn.q_type,
                            language_code=lang_code,
                            answer_justification=answer_justification,
                            document_retrieval=BenchmarkDocumentRetrievalResult(
                                query=document_query,
                                result=document_results,
                                recall=document_retrieval_scores["recall"],
                                precision=document_retrieval_scores["precision"],
                                correct_at_rank_1=document_retrieval_scores["correct_at_rank_1"],
                                rephraser="completed" if rephrased_from_input else rag.rephraser_model.name,
                            ),
                        )
                )
                print_summary(conv, turn, question, answer, answer_justification, answer_rating, language_code=lang_code, document_query=document_query, rephraser_mode="completed" if rephrased_from_input else "rephraser")
            else:
                print("Not enough data to generate prompt.")

            conv.history[lang_code].append(RAGMessage(author=Author.USER, content=question))
            conv.history[lang_code].append(RAGMessage(author=Author.ASSISTANT, content=answer))

    return conversation_benchmark_results


async def run_document_benchmark(
    rag: RAG,
    data: List[Conversation],
    repetitions: int = 1,
    languages: Optional[List[str]] = None,
    rephrased_from_input: bool = False,
    delay: float = 0.0,
    output_filename: Optional[Path] = None
) -> Dict[str, List[BenchmarkConversationResult]]:
    results = {}

    for conv in data:
        result = await run_conversation_benchmark(conv, rag, languages, rephrased_from_input, delay)
        results[result[0].conv_id] = result

        if output_filename:
            output_filename.parent.mkdir(parents=True, exist_ok=True)

            results_dict = {
                conv_id: [dataclasses.asdict(r) for r in conv_results]
                for conv_id, conv_results in results.items()
            }

            with open(output_filename, "w", encoding="utf-8") as f:
                json.dump(results_dict, f, ensure_ascii=False, indent=4)

    return results


def store_results(results: Dict[str, List[BenchmarkConversationResult]], output_file: Path, answer_generation_model: Optional[str] = "GPT-4o"):
    rows = []
    for key, conv in results.items():
        for result in conv:
            row = dict(
                conv_id=result.conv_id,
                turn_id=result.turn_id,
                q=result.question,
                a=result.a,
                lang=result.language_code,
                a_gen=result.a_gen,
            )

            if result.document_retrieval is not None:
                row.update(dict(
                    document_query=result.document_retrieval.query,
                    evidences=result.document_retrieval.result,
                    retr_recall=result.document_retrieval.recall,
                    retr_prec=result.document_retrieval.precision,
                    retr_correct_at_rank_1=result.document_retrieval.correct_at_rank_1,
                    rephraser=result.document_retrieval.rephraser,
                ))

            row.update(dict(
                a_rel=result.answer_right,
                a_just=result.answer_justification,
                a_source=result.a_source,
                q_type=result.q_type,
                model=str(answer_generation_model)
            ))

            rows.append(row)
    df = pd.DataFrame(rows)
    if output_file.suffix == ".csv":
        df.to_csv(output_file, index=False)
    elif output_file.suffix == ".xlsx":
        df.to_excel(output_file, index=False)

    nested_dict = (
        df.groupby("conv_id")
        .apply(lambda x: {
            "conv_id": str(x.name),
            "turns": x.drop(columns="conv_id").to_dict(orient="records")
        })
        .tolist()
    )
    with open(str(output_file.parent / output_file.stem) + ".json", "w", encoding="utf-8") as json_file:
        json.dump(nested_dict, json_file, indent=4)
        print(f"Formatted JSON written to: {str(output_file.parent / output_file.stem) + '.json'}")

    print(f"Results written to: {str(output_file.parent)}")


def load_benchmark_data(input_file: Path):
    with open(input_file, "r") as f:
        data = json.load(f)
    return TypeAdapter(List[Conversation]).validate_python(data)


def readable_config_string(config: dict) -> str:
    verbalizer = config.get("verbalizer_config", {})
    modalities = config.get("modalities", [])
    mode = "-".join(verbalizer.get("mode", []))
    granularity = "-".join(verbalizer.get("granularity", []))
    modalities_str = "-".join(modalities)

    readable_str = f"{mode}_{granularity}_{modalities_str}"
    return readable_str.replace(" ", "-").lower()


def create_new_folder(base_path: Path, readable_str: str) -> Path:
    """Create a new folder with a readable string and handle name conflicts."""
    folder_path = base_path / readable_str
    original_folder_path = folder_path

    count = 1
    while folder_path.exists():
        folder_path = original_folder_path.with_name(f"{readable_str}_{count}")
        count += 1

    folder_path.mkdir(parents=True)
    return folder_path


def process_config_and_create_folder(config_json_path: Path, export_file_path: Path, rephrased_from_input: bool = False) -> Path:
    """Read config, create folder, copy config, and return new path for the export file."""
    with open(config_json_path, 'r') as f:
        config = json.load(f)

    readable_str = readable_config_string(config) + f"_rephrasedFromInput-{str(rephrased_from_input)}"

    base_path = export_file_path.parent
    new_folder = create_new_folder(base_path, readable_str)

    new_config_path = new_folder / config_json_path.name
    shutil.copy(config_json_path, new_config_path)

    new_file_path = new_folder / export_file_path.name

    return new_file_path


def run_benchmark(
    input_file: Path,
    rag: RAG,
    database: Database, # to be compatible with http_server yaml
    output_file: Path,
    languages: Optional[List[Literal["en", "de"]]] = None,
    document_database_grid: Optional[List[DocumentDatabase]] = None,
    repetitions: int = 1,
    rephrased_from_input: bool = False,
    delay: float = 0.0,
):
    async def main():
        data = load_benchmark_data(input_file)

        if document_database_grid:
            for document_database in document_database_grid:
                rag.document_database = document_database
                result = await run_document_benchmark(rag, data, repetitions, languages, rephrased_from_input, delay)
                organized_output_file = process_config_and_create_folder(document_database.file / Path("multi_modal_config.json"), output_file, rephrased_from_input)
                store_results(result, organized_output_file, rag.answer_generation_model)
                time.sleep(delay)
        else:
            result = await run_document_benchmark(rag, data, repetitions, languages, rephrased_from_input, output_filename=output_file.parent / "result_intermediate.json")
            store_results(result, output_file, rag.answer_generation_model.name)

    asyncio.run(main())


if __name__ == "__main__":
    from jsonargparse import CLI
    CLI(run_benchmark, as_positional=False)
